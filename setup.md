

## ğŸ§© **setup.md â€” Retail Demand Forecasting MLOps Pipeline**

### ğŸ“˜ **Overview**

This document explains how to set up and run the **Retail Demand Forecasting MLOps Pipeline** â€” an end-to-end workflow that predicts product demand, detects model drift, retrains models, and redeploys automatically using free-tier tools.

---

## âš™ï¸ **1. Prerequisites**

Before setup, ensure you have:

| Requirement              | Version  | Purpose                 |
| ------------------------ | -------- | ----------------------- |
| **Python**               | â‰¥ 3.10   | Core development        |
| **Git**                  | latest   | Version control         |
| **Docker**               | latest   | Containerization        |
| **GitHub account**       | â€”        | CI/CD automation        |
| **DagsHub account**      | â€”        | DVC remote storage      |
| **Hugging Face account** | â€”        | API deployment (Spaces) |
| **Grafana & Prometheus** | optional | Monitoring stack        |

---

## ğŸ“¦ **2. Project Structure**



```
retail-demand-forecasting-mlops/
â”‚
â”œâ”€â”€ .dvc/
â”‚   â”œâ”€â”€ .gitignore            # Git ignore settings for DVC
â”‚   â””â”€â”€ config                # DVC configuration file
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ retrain.yml       # GitHub Actions workflow for retraining
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ .gitignore        # Git ignore settings for processed data
â”‚   â”‚   â”œâ”€â”€ cleaned_data.csv.dvc  # DVC-tracked cleaned data file
â”‚   â”‚   â””â”€â”€ feature_data.csv  # Feature-engineered data
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ sample_submission.csv  # Sample submission file
â”‚       â”œâ”€â”€ test.csv              # Test dataset
â”‚       â””â”€â”€ train.csv             # Training dataset
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ drift_logs.log         # Log for drift detection
â”‚   â””â”€â”€ retrain_logs.log       # Log for retraining pipeline
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ .gitignore             # Git ignore settings for models
â”‚   â”œâ”€â”€ metrics.json           # Model evaluation metrics
â”‚   â””â”€â”€ model.pkl.dvc          # DVC-tracked model file
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preparation.ipynb  # Notebook for data preparation
â”‚   â””â”€â”€ 02_drift_detection.ipynb   # Notebook for drift detection
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ data_drift_report.html  # HTML report for data drift
â”‚   â””â”€â”€ drift_summary.json      # Summary of drift analysis
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py             # FastAPI app for model serving
â”‚   â”‚   â”œâ”€â”€ predict.py         # Prediction logic for the API
â”‚   â”‚   â””â”€â”€ utils.py           # Helper functions for model pulling
â”‚   â”œâ”€â”€ drift/
â”‚   â”‚   â””â”€â”€ drift_detection.py  # Drift detection logic
â”‚   â””â”€â”€ retraining/
â”‚       â”œâ”€â”€ retrain_pipeline.py  # Retraining pipeline
â”‚       â””â”€â”€ test.py             # Unit tests for retraining pipeline
â”‚
â”œâ”€â”€ .dvcignore                 # Ignore file for DVC
â”œâ”€â”€ .gitattributes             # Git attributes for handling files
â”œâ”€â”€ .gitignore                 # Git ignore settings
â”œâ”€â”€ Dockerfile                 # Dockerfile for containerization
â”œâ”€â”€ LICENSE                    # Project license
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ docker-compose.yml         # Docker Compose setup
â”œâ”€â”€ drift_result.txt           # Text file for drift results
â”œâ”€â”€ dvc.yaml                   # DVC pipeline configuration
â”œâ”€â”€ load_test.py               # Script for load testing
â”œâ”€â”€ params.yaml                # Configuration parameters
â”œâ”€â”€ prometheus.yml             # Prometheus configuration for monitoring
â””â”€â”€ requirements.txt           # Project dependencies
```




---

## ğŸ§± **3. Environment Setup**

### ğŸ§° Step 3.1 â€” Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # for Linux/Mac
venv\Scripts\activate     # for Windows
```

### ğŸ“¦ Step 3.2 â€” Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§‘â€ğŸ’» **4. Local Development Workflow**

### ğŸ”¹ Data Preparation

```bash
python src/data_prep.py
```

Cleans raw data, creates time-based and lag features, and stores processed data under `data/processed/`.

### ğŸ”¹ Model Training

```bash
python src/train_model.py
```

Trains the model, evaluates metrics (MAE, RMSE, MAPE), and saves it as `models/model_latest.pkl`.

### ğŸ”¹ Drift Detection

```bash
python src/drift_detection.py
```

Runs data & concept drift detection using **Evidently AI** and logs results to console or JSON.

---

## ğŸš€ **5. API Deployment (FastAPI)**

### ğŸ§© Step 5.1 â€” Run Locally

```bash
uvicorn src.api.app:app --reload
```

API available at â†’ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
Test `/predict` endpoint with JSON input.

### ğŸ³ Step 5.2 â€” Docker Build

```bash
docker build -t retail-forecast-api .
docker run -p 8000:8000 retail-forecast-api
```

### â˜ï¸ Step 5.3 â€” Deploy on Hugging Face Spaces

1. Create a new **Space** (Type: Docker).
2. Push repo to Hugging Face:

   ```bash
   git remote add space https://huggingface.co/spaces/<USERNAME>/<SPACE_NAME>
   git push space main
   ```
3. Hugging Face automatically builds and deploys your container.

---

## ğŸ“Š **6. Monitoring Setup (Prometheus + Grafana)**

### ğŸ³ Step 6.1 â€” Launch Monitoring Stack

```bash
docker-compose up --build
```

Access:

* **FastAPI** â†’ [http://localhost:8000](http://localhost:8000)
* **Prometheus** â†’ [http://localhost:9090](http://localhost:9090)
* **Grafana** â†’ [http://localhost:3000](http://localhost:3000) (admin / admin)

### ğŸ” Step 6.2 â€” Grafana Setup

* Add Prometheus as a data source (`http://prometheus:9090`)
* Create dashboards using metrics:

  * `request_count`
  * `prediction_count`
  * `request_latency_seconds`

---

## âš™ï¸ **7. CI/CD Automation (GitHub Actions)**

The pipeline is defined in `.github/workflows/ci_cd_pipeline.yml`.

### Workflow Steps:

1. Pull latest data/model from DagsHub (DVC)
2. Retrain model automatically
3. Push updated model to DagsHub
4. Redeploy Hugging Face Space

### Required GitHub Secrets:

| Secret          | Description               |
| --------------- | ------------------------- |
| `DAGSHUB_TOKEN` | Access token from DagsHub |
| `DAGSHUB_USER`  | Your DagsHub username     |
| `HF_TOKEN`      | Hugging Face Access Token |
| `HF_USERNAME`   | Hugging Face username     |
| `HF_SPACE`      | Hugging Face Space name   |

---

## ğŸ§¾ **8. Reproducibility Checklist**

| Component        | Tool                         | Configured | Verified |
| ---------------- | ---------------------------- | ---------- | -------- |
| Data Versioning  | DVC + DagsHub                | âœ…          | âœ…        |
| Model Versioning | DVC                          | âœ…          | âœ…        |
| Drift Detection  | Evidently AI                 | âœ…          | âœ…        |
| CI/CD            | GitHub Actions               | âœ…          | âœ…        |
| Deployment       | FastAPI + Docker + HF Spaces | âœ…          | âœ…        |
| Monitoring       | Prometheus + Grafana         | âœ…          | âœ…        |

---

## ğŸ§  **9. Notes for Reproducibility**

* All scripts are deterministic with `random_state=42`.
* Each stage (data, model, drift, deployment) is modular and can be run independently.
* To reproduce a previous version, checkout the corresponding Git commit and use:

  ```bash
  dvc checkout
  ```
* All pipeline components use **only free-tier services**.

---

## ğŸ **10. Project Summary**

| Stage            | Tool                                   | Description                   |
| ---------------- | -------------------------------------- | ----------------------------- |
| Data Preparation | Pandas, Scikit-learn                   | Feature engineering           |
| Model Training   | XGBoost / RandomForest                 | Regression-based forecasting  |
| Drift Detection  | Evidently AI                           | Data + concept drift tracking |
| Retraining       | DVC + GitHub Actions                   | Automated on change           |
| Deployment       | FastAPI + Docker + Hugging Face Spaces | API serving                   |
| Monitoring       | Prometheus + Grafana                   | Metrics visualization         |

---

âœ… **Setup complete!**
This `setup.md` ensures anyone can **replicate, run, and maintain** your retail demand forecasting MLOps pipeline with no paid services.

